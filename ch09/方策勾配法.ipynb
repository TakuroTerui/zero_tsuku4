{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "方策勾配法.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMI7ff301JEkbxvM9S6+qeW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFSWmbMEcV4d",
        "outputId": "7accc660-9aa5-44b3-e3c6-2de8b9e096cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dezero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVG7ATDkcZ6p",
        "outputId": "dfd33e44-698a-4909-b804-7baee4f50308"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dezero\n",
            "  Downloading dezero-0.0.13-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dezero) (1.21.5)\n",
            "Installing collected packages: dezero\n",
            "Successfully installed dezero-0.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6mxvGRQQcHDj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from dezero import Model\n",
        "from dezero import optimizers\n",
        "import dezero.functions as F\n",
        "import dezero.layers as L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(Model):\n",
        "    def __init__(self, action_size):\n",
        "        super().__init__()\n",
        "        self.l1 = L.Linear(128)\n",
        "        self.l2 = L.Linear(action_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.softmax(self.l2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "EubYFBVlcpf4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.98\n",
        "        self.lr = 0.0002\n",
        "        self.action_size = 2\n",
        "\n",
        "        self.memory = []\n",
        "        self.pi = Policy(self.action_size)\n",
        "        self.optimizer = optimizers.Adam(self.lr)\n",
        "        self.optimizer.setup(self.pi)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = state[np.newaxis, :] # バッチの軸を追加\n",
        "        probs = self.pi(state)\n",
        "        probs = probs[0]\n",
        "        action = np.random.choice(len(probs), p=probs.data)\n",
        "        return action, probs[action]"
      ],
      "metadata": {
        "id": "etTabTjZdLgc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "state = env.reset()\n",
        "agent = Agent()\n",
        "\n",
        "action, prob = agent.get_action(state)\n",
        "print('action:', action)\n",
        "print('prob:', prob)\n",
        "\n",
        "G = 100.0 # ダミーの重み\n",
        "J = G * F.log(prob)\n",
        "print('J:', J)\n",
        "\n",
        "# 勾配を求める\n",
        "J.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll4TZFxOd-Ow",
        "outputId": "ef8d3e3d-7d18-4714-f3f5-fb40fc28e7cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action: 1\n",
            "prob: variable(0.48890732087187133)\n",
            "J: variable(-71.55823353393251)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.98\n",
        "        self.lr = 0.0002\n",
        "        self.action_size = 2\n",
        "\n",
        "        self.memory = []\n",
        "        self.pi = Policy(self.action_size)\n",
        "        self.optimizer = optimizers.Adam(self.lr)\n",
        "        self.optimizer.setup(self.pi)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = state[np.newaxis, :] # バッチの軸を追加\n",
        "        probs = self.pi(state)\n",
        "        probs = probs[0]\n",
        "        action = np.random.choice(len(probs), p=probs.data)\n",
        "        return action, probs[action]\n",
        "    \n",
        "    def add(self, reward, prob):\n",
        "        data = (reward, prob)\n",
        "        self.memory.append(data)\n",
        "    \n",
        "    def update(self):\n",
        "        self.pi.cleargrads()\n",
        "\n",
        "        G, loss = 0, 0\n",
        "        for reward, prob in reversed(self.memory):\n",
        "            G = reward + self.gamma * G\n",
        "\n",
        "        for reward, prob in self.memory:\n",
        "            loss += -F.log(prob) * G\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.update()\n",
        "        self.memory = [] # メモリをリセット"
      ],
      "metadata": {
        "id": "tb15kZmGecFW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 3000\n",
        "env = gym.make('CartPole-v0')\n",
        "agent = Agent()\n",
        "reward_history = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action, prob = agent.get_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        agent.add(reward, prob)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    \n",
        "    agent.update()\n",
        "    reward_history.append(total_reward)"
      ],
      "metadata": {
        "id": "KU5JkS55fP4s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REINFORCEの実装"
      ],
      "metadata": {
        "id": "VJBM0MQQg4l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.98\n",
        "        self.lr = 0.0002\n",
        "        self.action_size = 2\n",
        "\n",
        "        self.memory = []\n",
        "        self.pi = Policy(self.action_size)\n",
        "        self.optimizer = optimizers.Adam(self.lr)\n",
        "        self.optimizer.setup(self.pi)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = state[np.newaxis, :] # バッチの軸を追加\n",
        "        probs = self.pi(state)\n",
        "        probs = probs[0]\n",
        "        action = np.random.choice(len(probs), p=probs.data)\n",
        "        return action, probs[action]\n",
        "    \n",
        "    def add(self, reward, prob):\n",
        "        data = (reward, prob)\n",
        "        self.memory.append(data)\n",
        "    \n",
        "    def update(self):\n",
        "        self.pi.cleargrads()\n",
        "\n",
        "        G, loss = 0, 0\n",
        "        for reward, prob in reversed(self.memory):\n",
        "            G = reward + self.gamma * G\n",
        "            loss += -F.log(prob) * G\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.update()\n",
        "        self.memory = [] # メモリをリセット"
      ],
      "metadata": {
        "id": "oo1QWe-hhY8a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor-Criticの実装"
      ],
      "metadata": {
        "id": "e1Pvel5ZhkE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from dezero import Model\n",
        "from dezero import optimizers\n",
        "import dezero.functions as F\n",
        "import dezero.layers as L\n",
        "\n",
        "class PolicyNet(Model):\n",
        "    def __init__(self, action_size=2):\n",
        "        super().__init__()\n",
        "        self.l1 = L.Linear(128)\n",
        "        self.l2 = L.Linear(action_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = self.l2(x)\n",
        "        x = F.softmax(x)\n",
        "        return x\n",
        "\n",
        "class ValueNet(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = L.Linear(128)\n",
        "        self.l2 = L.Linear(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = self.l2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "v9BJugsOidjZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.98\n",
        "        self.lr_pi = 0.0002\n",
        "        self.lr_v = 0.0005\n",
        "        self.action_size = 2\n",
        "\n",
        "        self.pi = PolicyNet()\n",
        "        self.v = ValueNet()\n",
        "        self.optimizer_pi = optimizers.Adam(self.lr_pi).setup(self.pi)\n",
        "        self.optimizer_v = optimizers.Adam(self.lr_v).setup(self.v)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        state = state[np.newaxis, :] # バッチの軸を追加\n",
        "        probs = self.pi(state)\n",
        "        probs = probs[0]\n",
        "        action = np.random.choice(len(probs), p=probs.data)\n",
        "        return action, probs[action]\n",
        "    \n",
        "    def update(self, state, action_prob, reward, next_state, done):\n",
        "        # バッジ軸の追加\n",
        "        state = state[np.newaxis, :]\n",
        "        next_state = next_state[np.newaxis, :]\n",
        "\n",
        "        # ①self.vの損失\n",
        "        target = reward + self.gamma * self.v(next_state) * (1 - done)\n",
        "        target.upchain()\n",
        "        v = self.v(state)\n",
        "        loss_v = F.mean_squared_error(v, target)\n",
        "\n",
        "        # ②self.piの損失\n",
        "        delta = target - v\n",
        "        delta.unchain()\n",
        "        loss_pi = -F.log(action_prob) * delta\n",
        "\n",
        "        self.v.cleargrads()\n",
        "        self.pi.cleargrads()\n",
        "        loss_v.backward()\n",
        "        loss_pi.backward()\n",
        "        self.optimizer_v.update()\n",
        "        self.optimizer_pi.update()"
      ],
      "metadata": {
        "id": "B0p9gnNbjYB_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 方策ベースの手法の利点\n",
        "1. 方策を直接モデル化するので効率的\n",
        "2. 連続的な行動空間でも使える\n",
        "3. 行動の選択確率がスムーズに変化する"
      ],
      "metadata": {
        "id": "5avF6BCnlFaB"
      }
    }
  ]
}