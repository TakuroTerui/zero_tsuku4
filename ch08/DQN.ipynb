{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlGHarFBcrMhTUyMR8DAOp"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYnimait-PVZ",
        "outputId": "8f7f053b-743e-483f-e0fe-d58f006fdfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v0')"
      ],
      "metadata": {
        "id": "nJHYy5Nc-VkC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "print(state) # 初期状態\n",
        "\n",
        "action_space = env.action_space\n",
        "print(action_space) # 行動の次元数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV7_021E-d7t",
        "outputId": "fccca8cd-40d9-4a4e-e089-43ac097ab616"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.03185282  0.02733258 -0.03137989  0.01263766]\n",
            "Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- カートの位置\n",
        "- カートの速度\n",
        "- 棒の角度\n",
        "- 棒の角速度"
      ],
      "metadata": {
        "id": "aVGCtcsV-1Bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action = 0 # 0 or 1\n",
        "next_state, reward, done, info = env.step(action)\n",
        "print(next_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00MwcfQs_CPq",
        "outputId": "0320a56e-0be2-4382-99ed-a15c31a10c1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.03130617 -0.16732562 -0.03112714  0.29525703]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "env.step(action)の結果\n",
        "- 次の状態（next_state）\n",
        "- 報酬（reward）\n",
        "- 終了かどうかのフラグ（done）\n",
        "- 追加の情報（info）"
      ],
      "metadata": {
        "id": "ICSToHg__SNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ランダムなエージェント"
      ],
      "metadata": {
        "id": "tzbxwoOc_py7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # env.render()\n",
        "    action = np.random.choice([0, 1])\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "env.close()"
      ],
      "metadata": {
        "id": "FjQYZ5VC_wgW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 経験再生の実装"
      ],
      "metadata": {
        "id": "WKUeFwgzAGDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        data = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(data)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "    \n",
        "    def get_batch(self):\n",
        "        data = random.sample(self.buffer, self.batch_size)\n",
        "\n",
        "        state = np.stack([x[0] for x in data])\n",
        "        action = np.stack([x[1] for x in data])\n",
        "        reward = np.stack([x[2] for x in data])\n",
        "        next_state = np.stack([x[3] for x in data])\n",
        "        done = np.array([x[4] for x in data]).astype(np.int32)\n",
        "        return state, action, reward, next_state, done"
      ],
      "metadata": {
        "id": "vREiWthkBprc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "replay_buffer = ReplayBuffer(buffer_size=10000, batch_size=32)\n",
        "\n",
        "for episode in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = 0\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        replay_buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "    \n",
        "state, action, reward, next_state, done = replay_buffer.get_batch()\n",
        "print(state.shape)\n",
        "print(action.shape)\n",
        "print(reward.shape)\n",
        "print(next_state.shape)\n",
        "print(done.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVQVfu12Ctmp",
        "outputId": "6d3a2f8e-92f0-403f-9c9e-fa0b93ad9eb4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 4)\n",
            "(32,)\n",
            "(32,)\n",
            "(32, 4)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ターゲットネットワークの実装"
      ],
      "metadata": {
        "id": "rGL1UzgyD8tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dezero"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9TDptfDEwFa",
        "outputId": "9b05fa34-bf98-45ef-eb68-6263b8a67ca6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dezero\n",
            "  Downloading dezero-0.0.13-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from dezero) (1.21.5)\n",
            "Installing collected packages: dezero\n",
            "Successfully installed dezero-0.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from dezero import Model\n",
        "from dezero import optimizers\n",
        "import dezero.functions as F\n",
        "import dezero.layers as L\n",
        "\n",
        "class QNet(Model):\n",
        "    def __init__(self, action_size):\n",
        "        super().__init__()\n",
        "        self.l1 = L.Linear(128)\n",
        "        self.l2 = L.Linear(128)\n",
        "        self.l3 = L.Linear(action_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.l3(x)\n",
        "        return x\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.gamma = 0.98\n",
        "        self.lr = 0.0005\n",
        "        self.epsilon = 0.1\n",
        "        self.buffer_size = 10000\n",
        "        self.batch_size = 32\n",
        "        self.action_size = 2\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
        "        self.qnet = QNet(self.action_size)\n",
        "        self.qnet_target = QNet(self.action_size)\n",
        "        self.optimizer = optimizers.Adam(self.lr)\n",
        "        self.optimizer.setup(self.qnet) # qnetを設定\n",
        "    \n",
        "    def sync_qnet(self):\n",
        "        self.qnet_target = copy.deepcopy(self.qnet)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        else:\n",
        "            state = state[np.newaxis, :] # バッチの次元を追加\n",
        "            qs = self.qnet(state)\n",
        "            return qs.data.argmax()\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "        \n",
        "        state, action, reward, next_state, done = self.replay_buffer.get_batch()\n",
        "        qs = self.qnet(state) # ①\n",
        "        q = qs[np.arange(self.batch_size), action] # ②\n",
        "\n",
        "        next_qs = self.qnet_target(next_state) # ③\n",
        "        next_q = next_qs.max(axis=1)\n",
        "        next_q.unchain()\n",
        "        target = reward + (1 - done) * self.gamma * next_q # ④\n",
        "\n",
        "        loss = F.mean_squared_error(q, target)\n",
        "\n",
        "        self.qnet.cleargrads()\n",
        "        loss.backward()\n",
        "        self.optimizer.update()"
      ],
      "metadata": {
        "id": "3wPt9uZNEmPJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQNを動かす"
      ],
      "metadata": {
        "id": "TqLaQpp-GcyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 300\n",
        "sync_interval = 20\n",
        "env = gym.make('CartPole-v0')\n",
        "agent = DQNAgent()\n",
        "reward_history = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        agent.update(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    \n",
        "    if episode % sync_interval == 0:\n",
        "        agent.sync_qnet()\n",
        "    \n",
        "    reward_history.append(total_reward)"
      ],
      "metadata": {
        "id": "DwR_WpX9H2LQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# greedyな行動\n",
        "agent.epsilon = 0 # greedy policy\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "    # env.render()\n",
        "print('Total Reward:', total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dQJ7udPIpV5",
        "outputId": "6c03b94a-5d35-4d78-c02b-a381eda41264"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: 178.0\n"
          ]
        }
      ]
    }
  ]
}